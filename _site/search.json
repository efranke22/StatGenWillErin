{
  "articles": [
    {
      "path": "analysisR.html",
      "title": "Data Analysis",
      "description": "More on multiple hypothesis testing on how to do it in RStudio with the HapMap data.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nThe limit of Bonferroni\r\nDetermining a threshold with simulation in RStudio\r\nStep One\r\nStep Two\r\nStep Three\r\nStep Four\r\n\r\nA computationally efficient solution\r\n\r\nThis page will go through an example of what the multiple hypothesis testing procedure looks like in RStudio with the HapMap data. Before reviewing this section, please visit the Data tab to learn more about the dataset we are using the cleaning steps necessary to perform this analysis as well as load the data into your RStudio environment.\r\n\r\n\r\n\r\nThe limit of Bonferroni\r\nAs discussed in the Hypothesis Testing Background section, multiple hypothesis testing is a necessary part of statistical analyses. It helps us determine the correct threshold for selecting SNPs to devote further time and resources to studying as using the standard 0.05 threshold would lead to falsely asserting relationships between a SNP and the trait of interest far too often (too many Type I errors).\r\nWhile Bonferroni is a great way to get an approximate threshold for a given number of SNPs, it is flawed because it treats all SNPs as independent of one another. In reality, SNPs are correlated with each other and the extent of that correlation depends on the data. This is known as the concept of linkage disequalibrium, which as stated in Science Direct (‚ÄúLinkage Disequilibrium,‚Äù n.d.) is the idea that two markers in close physical proximity are correlated in a population and are in association more than would be expected with random assortment. Essentially, SNPs next to each other are much more similar than SNPs far away from each other.\r\nWe can show this concept of correlation with our data. To do this, we will use the SnpMatrix in which our data is stored. As mentioned in the Data section, each row is one of the 165 members of this study, and each column is one of our 1,457,897 SNPs.\r\n\r\n\r\nhapmap$genotypes\r\n\r\nA SnpMatrix with  165 rows and  1457897 columns\r\nRow names:  NA06989 ... NA12865 \r\nCol names:  rs2185539 ... rs1973881 \r\n\r\n1,457,897 SNPs is a big number, and it would be hard to show the correlation when all of them. As a result, we just show the linkage disequalibrium matrix below for the first 100 polymorphic SNPs on chromosome 1. The diagonal line moving from the top left corner of the plot to the bottom right corner represents each SNP‚Äôs correlation with itself, and therefore is filled in as white square. What you can notice from this plot is that SNPs that are nearby each other (bordering the white diagonal line) are often represented in orange, meaning they are highly correlated.\r\n\r\n\r\nchr1_100 <- hapmap$genotypes[1:165, 1:119]\r\ncolor.pal <- natparks.pals(\"Acadia\", 10)\r\n\r\n#get monomorphic SNPs only\r\nmaf_chr1_100 <- col.summary(chr1_100)$MAF\r\nmono <- which(maf_chr1_100 == 0)\r\n\r\n# calculate LD on polymorphic SNPs only\r\nhapmap.ld.nomono <- ld(chr1_100[,-mono], depth = 118-length(mono), stats = \"R.squared\", symmetric = TRUE)\r\n\r\n# plot \r\nimage(hapmap.ld.nomono, lwd = 0, cuts = 9, col.regions = color.pal, colorkey = TRUE)\r\n\r\n\r\n\r\nWhile the first 100 polymorphic SNPs from this HapMap dataset have the linkage disequilibrium matrix shown above, if we were to look at another dataset it might look completely different. For example, the linkage disequilibrium matrix for the first 100 SNPs from a dataset from Rbloggers (Lima 2017) looks like this:\r\n\r\n\r\n\r\nThe reason we are showing these correlation matrices is to demonstrate that different studies have different levels of correlation in the data. The more similar SNPs are, the fewer hypothesis tests effectively conducted and thus the higher the threshold can be. Therefore, even with the same number of SNPs in two different studies, thresholds can vary quite a bit. The method to determine the right threshold for your data therefore must be done using simulation.\r\nDetermining a threshold with simulation in RStudio\r\nThe process for determining a threshold is as follows:\r\nSimulate a null trait, meaning a trait not associated with any of the SNPs.\r\nRun GWAS to test the association between the simulated null trait and each SNP in our dataset. After that record the smallest p-value from this GWAS.\r\nRepeat steps 1 and 2 many times, typically 1,000-10,000 times in professional genetic studies.\r\nLook at the p-values saved from those simulation replicates. Sort them from smallest to largest and find the number at which 5% (desired FWER) of p-values are smaller than that number. This is the significance threshold.\r\nWe will break this down step by step.\r\nStep One\r\nThe first step is simulate a null trait, meaning a trait not associated with any SNPs. We call this trait y, and generate 165 data points with a mean of 0 and standard deviation of 1.\r\n\r\n\r\nset.seed(494)\r\ny = rnorm(n = 165, mean = 0, sd = 1)\r\n\r\n\r\nStep Two\r\nNext, we run a GWAS to test the association between the simulated null trait y and each SNP in our dataset. To do this, we use marginal regression and fit a model with the SNP as the single independent variable and the trait of interest as the dependent variable. Looking at our first three SNPs, the models can be created as shown in the code chunk below. If you don‚Äôt have X.clean in your R environment, go run the code in the Data section.\r\n\r\n\r\nset.seed(494)\r\nsnp1mod <- lm(y ~ X.clean[,1])\r\nsnp2mod <- lm(y ~ X.clean[,2])\r\nsnp3mod <- lm(y ~ X.clean[,3])\r\n\r\ntidy(snp1mod)\r\n\r\n# A tibble: 2 √ó 5\r\n  term         estimate std.error statistic p.value\r\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)   -0.203      0.929    -0.218   0.828\r\n2 X.clean[, 1]   0.0939     0.466     0.202   0.841\r\n\r\nEach of these models produces an estimate for the coefficient on the SNP. For example, the coefficient for snp1mod is 0.094. The way we might interpret this is that for every additional minor allele (G for example) that you carry at that position, the trait of interest changes by about 0.094 units. If the trait we were measuring was height, we would expect your height to increase about 0.094 inches for every additional minor allele (a value of either 0, 1, or 2) at SNP 1.\r\nObviously, we cannot do the process above by hand for over one million SNPs (which is necessary to complete 1 GWAS). However, we can do this with a loop! This code loops through each of the SNPs, fitting a linear regression model at each one. For each model, we record the estimates (betas), standard errors (ses), test statistics (tstats) and p-values (pvals) for the coefficient of interest, which is the slope.\r\nWarning: this code may take 30-60 minutes to run. Feel free to read over the rest of this step but skip running any code.\r\n\r\n\r\n# set up empty vectors for storing results\r\nbetas <- c()\r\nses <- c()\r\ntstats <- c()\r\npvals <- c()\r\n\r\n# loop through all SNPs\r\nfor(i in 1:ncol(X.clean)){ \r\n  # fit model\r\n  mod <- lm(y ~ X.clean[,i])\r\n  # get coefficient information\r\n  coefinfo <- tidy(mod)\r\n  # record estimate, SE, test stat, and p-value\r\n  betas[i] <- coefinfo$estimate[2]\r\n  ses[i] <- coefinfo$std.error[2]\r\n  tstats[i] <- coefinfo$statistic[2]\r\n  pvals[i] <- coefinfo$p.value[2]\r\n}\r\n\r\n\r\nNext we record add our results to our map.clean data frame that contains information about each SNP:\r\n\r\n\r\nall.results <- map.clean %>%\r\n  mutate(Estimate = betas,\r\n         Std.Error = ses,\r\n         Test.Statistic = tstats,\r\n         P.Value = pvals)\r\n\r\nhead(all.results)\r\n\r\n\r\n\r\nWe then arrange the p-values from smallest to largest and record the smallest one. In this trial, the smallest p-value was \\(4 \\times 10^{-7}\\).\r\nIf this procedure was done with our trait of interest, an additional step to complete this GWAS might be to create a Manhattan plot which shows the p-values of all SNPs in our dataset. If a p-value is less than a threshold we have set, it should stand out on the plot and prompt further analysis. We will talk more about these Manhattan plots later on, once we have completed multiple hypothesis testing and determined a threshold.\r\nStep Three\r\nStep 3 is to repeat steps 1 and 2 many times, typically 1,000-10,000 times in professional genetic studies. But wait - did you run the GWAS above on your computer? If so, it probably took 30 - 60 minutes. In order to complete even 1000 replications at 30 minutes, it would take about 20.83 days on a single computer (we tested this and that number was accurate). 10,000 replications would take about 7 months, which we did not test (thankfully). As a result, this process is not feasible in RStudio! However, we will put the code below for 1000 replications in case you are tempted.\r\nOne thing this code does do is utilize the function mclapply() from the parallel package. This package will work to utilize all cores of your computer to the run code. The computer this code was run on only has two cores, but for computers with 8-10 cores this package could make a significant difference in computational time (perhaps on the level of 8-10x faster if nothing else is running in the background). However, even 8-10x faster will still result in it taking at least a few days to determine a threshold with 1000 replications.\r\n\r\n\r\ndim(X.clean)\r\ndo_one_sim<- function(i){\r\n  \r\n  # simulate null trait\r\n  y <- rnorm(n = 165, mean = 0, sd = 1) # n= number people in study\r\n  \r\n  # implement GWAS\r\n  pvals <- c()\r\n  for(i in 1:1283751){ #number SNPs in X.clean\r\n    mod <- lm(y ~ snp[,i])\r\n    pvals[i] <- tidy(mod)$p.value[2]\r\n  }\r\n  # record smallest p-value\r\n  min(pvals)\r\n}\r\n\r\n# Do 1000 replications with mclapply()\r\nset.seed(494)\r\nsimresmclapply <- mclapply(1:1000, do_one_sim, mc.cores = 2) \r\n\r\n\r\nStep Four\r\nIf you for some reason decided to run the code in Step 3, you could run the following code chunk to get the threshold for the family wise error rate of 5%. This is the significance threshold!\r\n\r\n\r\n# Print the 0.05 quantile \r\nquantile(simresmclapply %>% as.data.frame(), 0.05)\r\n\r\n\r\nA computationally efficient solution\r\nIn order to avoiding spending days of times and heavily comprising computer efficiency, we will demonstrate how to get threshold in PLINK. To learn more about PLINK and how to use it, check out the Hypothesis Testing in PLINK tab.\r\n\r\n\r\n\r\nLima, Francisco. 2017. ‚ÄúGenome-Wide Association Studies in r: R-Bloggers.‚Äù R. https://www.r-bloggers.com/2017/10/genome-wide-association-studies-in-r/.\r\n\r\n\r\n‚ÄúLinkage Disequilibrium.‚Äù n.d. Linkage Disequilibrium - an Overview | ScienceDirect Topics. https://www.sciencedirect.com/topics/neuroscience/linkage-disequilibrium.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-10T14:40:13-06:00"
    },
    {
      "path": "data.html",
      "title": "HapMap Data",
      "description": "This page explains information on the data context and cleaning steps necessary to run GWAS and multiple hypothesis testing procedures in RStudio.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nData Loading and Organization\r\nUnderstanding the Data\r\nData Cleaning\r\n\r\nIn the Data Analysis and Hypothesis Testing in PLINK tabs of this site, we will explain how to do a GWAS and determine a threshold for a set of genetic data. To follow along, download the 1_QC_GWAS.zip file from this page (MareesAT, n.d.). This data comes from the International HapMap project (also known as ‚ÄúHapMap‚Äù). For more information on the data and project, check out this tutorial (Marees et al. 2018).\r\nData Loading and Organization\r\nThe following code chunks outline the steps of importing the genetic data.\r\nIf you have not installed the snpStats package, install it in the console by running the following code chunk.\r\n\r\n\r\nif (!require(\"BiocManager\", quietly = TRUE))\r\n    install.packages(\"BiocManager\")\r\nBiocManager::install(\"snpStats\")\r\n\r\n\r\nLoad libraries:\r\n\r\n\r\nlibrary(snpStats)\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\nlibrary(NatParksPalettes)\r\nlibrary(parallel)\r\nlibrary(GGally)     \r\n\r\n\r\nLoad the data, using the correct directory of where you put the HapMap_3_r3_1.fam, HapMap_3_r3_1.bim, and HapMap_3_r3_1.bed files from the 1_QC_GWAS.zip folder. This process uses read.plink(), which reads a genotype matrix, information on the study‚Äôs individuals, and information on the SNPs.\r\n\r\n\r\nfam <- 'hapmapData/HapMap_3_r3_1.fam'\r\nbim <- 'hapmapData/HapMap_3_r3_1.bim'\r\nbed <- 'hapmapData/HapMap_3_r3_1.bed'\r\n\r\nhapmap <- read.plink(bed, bim, fam)\r\n\r\n\r\nUnderstanding the Data\r\nFirst, get information about the genotype data. We have 165 individuals and 1,457,897 SNPs.\r\n\r\n\r\nhapmap$genotypes\r\n\r\nA SnpMatrix with  165 rows and  1457897 columns\r\nRow names:  NA06989 ... NA12865 \r\nCol names:  rs2185539 ... rs1973881 \r\n\r\nNext, look at the information we have on the individuals in the study. Theoretically, this gives information on family relationships with pedigree, father, and mother, but the father and mother variables contain only missing values. We also have information on the individual‚Äôs binary sex, with 1 representing male and 2 female. The affected column represents if the individual had the trait of interest or not, but there are many missing values in this column.\r\n\r\n\r\nhead(hapmap$fam)\r\n\r\n        pedigree  member  father  mother sex affected\r\nNA06989     1328 NA06989    <NA>    <NA>   2        2\r\nNA11891     1377 NA11891    <NA>    <NA>   1        2\r\nNA11843     1349 NA11843    <NA>    <NA>   1        1\r\nNA12341     1330 NA12341    <NA>    <NA>   2        2\r\nNA12739     1444 NA12739 NA12748 NA12749   1       NA\r\nNA10850     1344 NA10850    <NA> NA12058   2       NA\r\n\r\nFinally, we can look at the information we have on each SNP. This tells us a few things:\r\nchromosome is the number chromosome (typically 1-23) that the SNP is located on.1 is the largest chromosome (most SNPs) and chromosome size typically decreases from there.\r\n\r\nsnp.name is the name of the SNP\r\ncM stands for centiMorgans, which is a unit for genetic distance. It represents an estimate of how far SNPs are from one another along the genome.\r\nposition tells us the base pair position of the SNP, with position being being the first nucleotide in our DNA sequence.This number restarts from 1 at each chromosome.\r\n\r\nallele.1 is one of the alleles at this SNP, here the minor allele.\r\nallele.2 is the other allele at this SNP, here the major allele.\r\n\r\n\r\nhead(hapmap$map)\r\n\r\n           chromosome   snp.name cM position allele.1 allele.2\r\nrs2185539           1  rs2185539 NA   556738        T        C\r\nrs11510103          1 rs11510103 NA   557616        G        A\r\nrs11240767          1 rs11240767 NA   718814        T        C\r\nrs3131972           1  rs3131972 NA   742584        A        G\r\nrs3131969           1  rs3131969 NA   744045        A        G\r\nrs1048488           1  rs1048488 NA   750775        C        T\r\n\r\nData Cleaning\r\nOne useful piece of information not contained in the data is the minor allele frequency (MAF), which represents what proportion of people have the less commonly occurring base pair in the data. We can add this to our snpMatrix using the snpstats package and add MAF to map, our data frame that gives us SNP information.\r\n\r\n\r\n#calculate MAF\r\nmaf <- col.summary(hapmap$genotypes)$MAF\r\n\r\n# add new MAF variable to map\r\nmap <- hapmap$map %>%\r\n  mutate(MAF = maf)\r\nhead(map)\r\n\r\n           chromosome   snp.name cM position allele.1 allele.2\r\nrs2185539           1  rs2185539 NA   556738        T        C\r\nrs11510103          1 rs11510103 NA   557616        G        A\r\nrs11240767          1 rs11240767 NA   718814        T        C\r\nrs3131972           1  rs3131972 NA   742584        A        G\r\nrs3131969           1  rs3131969 NA   744045        A        G\r\nrs1048488           1  rs1048488 NA   750775        C        T\r\n                  MAF\r\nrs2185539  0.00000000\r\nrs11510103 0.00621118\r\nrs11240767 0.00000000\r\nrs3131972  0.15757576\r\nrs3131969  0.13030303\r\nrs1048488  0.15853659\r\n\r\nJust looking at the MAF for the first six SNPs in our data, we see that in some cases the minor allele frequency is 0. This means that the SNP is monomorphic - everyone in the dataset has the same genotype at these positions. We will remove these monomorphic SNPs - if everyone has the same alleles at a SNP, there is no variation and we cannot find an association between the minor allele and the trait.\r\nIt can also help to think about why we remove SNPs with a MAF of 0 in a mathematical way. If we are trying to fit a line between the trait of interest and SNP 1, we could model this in the following formats, with linear regression listed first and matrix notation second.\r\n\\[E[Y|\\text{SNP1}] = \\beta_0 + \\beta1 \\text{SNP1}\\]\r\n\\[E[\\bf{y}|\\bf{X}] = \\boldsymbol{\\beta} X\\]\r\nFurther exploring the matrix format, it would look like this:\r\n\\[X\\boldsymbol{\\beta} = \\begin{bmatrix}\r\n1 & 0 \\\\\r\n1 & 0 \\\\\r\n. & . \\\\\r\n. & . \\\\\r\n\\end{bmatrix}\r\n\\begin{bmatrix}\r\n\\beta_0\\\\\r\n\\beta_1 \\\\\r\n\\end{bmatrix}\\]\r\nExecuting this multiplication, we just get \\(1 * \\beta_0 = 0\\). The is problematic because we have linear dependence. You can get the column of minor allele counts by multiplying the intercept column by 0 - in other words, the minor allele count column is a linear combination of the intercept column. This makes our design matrix not be full rank, making \\(X^TX\\) not invertible and the least squares estimator not defined.\r\nGiven all these reasons, we remove SNPs with a MAF of 0 using the code below.\r\n\r\n\r\nmap <- map %>%\r\n  filter(maf >0 )\r\n\r\ndim(map)\r\n\r\n[1] 1283751       7\r\n\r\nAfter filtering, we have 1,283,751 SNPs remaining. Therefore, we removed 174,146 monomorphic SNPs.\r\nBefore moving on, we must complete one final data cleaning step. The snpstats package uses a format in which genotypes are coded as 01, 02, and 03, with 00 representing missing values.\r\n\r\n\r\nhapmap$genotypes@.Data[1:5,1:5]\r\n\r\n        rs2185539 rs11510103 rs11240767 rs3131972 rs3131969\r\nNA06989        03         03         03        02        02\r\nNA11891        03         03         03        02        03\r\nNA11843        03         03         03        03        03\r\nNA12341        03         03         03        02        02\r\nNA12739        03         03         03        03        03\r\n\r\nWe will convert this to a 0, 1, and 2 format. Now the matrix represents the number of major alleles each person has at each SNP.\r\n\r\n\r\nX <- as(hapmap$genotypes, \"numeric\")\r\nX[1:5, 1:5]\r\n\r\n        rs2185539 rs11510103 rs11240767 rs3131972 rs3131969\r\nNA06989         2          2          2         1         1\r\nNA11891         2          2          2         1         2\r\nNA11843         2          2          2         2         2\r\nNA12341         2          2          2         1         1\r\nNA12739         2          2          2         2         2\r\n\r\nCreate X.clean by removing the monomorphic SNPs from X.\r\n\r\n\r\nmap.clean <- map %>%\r\n  filter(MAF >0)\r\nX.clean <- X[,colnames(X) %in% map.clean$snp.name]\r\n\r\n\r\nThe data is now clean and ready to be used. Make sure you keep it loaded in your environment for future use. Check out the Data Analysis tab next!\r\n\r\n\r\n\r\nMarees, Andries T, Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M Derks. 2018. ‚ÄúA Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.‚Äù International Journal of Methods in Psychiatric Research. U.S. National Library of Medicine. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/.\r\n\r\n\r\nMareesAT. n.d. ‚ÄúMareesAT/Gwa_tutorial: A Comprehensive Tutorial about GWAS and PRS.‚Äù GitHub. https://github.com/MareesAT/GWA_tutorial/.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-10T14:40:13-06:00"
    },
    {
      "path": "hapmapAnalysis.html",
      "title": "A final analysis of our HapMap data",
      "description": "How we combined our knowledge of multiple hypothesis testing and PLINK results to determine an appropriate significance threshold for our HapMap dataset\n",
      "author": [],
      "contents": "\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(janitor)\r\nlibrary(snpStats)\r\nlibrary(NatParksPalettes)\r\n\r\n\r\nLoad hapmap data\r\nCreate a null trait and write a file to the plink data folder.\r\n\r\n\r\nfam <- 'plinkData/HapMap_3_r3_1.fam'\r\nbim <- 'plinkData/HapMap_3_r3_1.bim'\r\nbed <- 'plinkData/HapMap_3_r3_1.bed'\r\n\r\nhapmap <- read.plink(bed, bim, fam)\r\n\r\n\r\n\r\n\r\nmanhattan_data <- cbind(hapmap$fam %>% select(1:2), trait = rnorm(n = 165, mean = 0, sd = 1))\r\nwrite_delim(manhattan_data, \"plinkData/manhattandata\")\r\n\r\n\r\nUse command to run GWAS in PLINK, accounting for correlated SNPs\r\n- ./plink ‚Äìbfile HapMap_3_r3_1 ‚Äìassoc ‚Äìadjust ‚Äìpheno manhattandata ‚Äìout as2\r\nRead in results from PLINK and create Manhattan plot\r\n\r\n\r\nresults <- read_table(\"plinkData/as2.qassoc.adjusted\")\r\n\r\nresults_with_position <- results %>%\r\n  mutate(CHR = as.integer(CHR)) %>%\r\n  left_join(hapmap$map %>%\r\n              select(snp.name, position, chromosome), by = c(\"SNP\" = \"snp.name\", \"CHR\" = \"chromosome\"))\r\n\r\n\r\n\r\n\r\nresults_with_position %>%\r\n  mutate(minuslogp = -log10(GC),\r\n         CHR = as.factor(CHR)) %>%\r\n  ggplot(aes(x = CHR, y = minuslogp, group = interaction(CHR, position), color = CHR)) + \r\n  geom_point(position = position_dodge(0.8)) + \r\n  labs(x = 'chromosome', y = expression(paste('-log'[10],'(p-value)')))+\r\n  theme_classic()+\r\n  scale_color_manual(values=natparks.pals(\"DeathValley\",24))+\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nComplete replications\r\nSimulation based approach. Approximately 14.45 minutes to run GWAS in plink, then 3 minutes 20 seconds to load files into R.\r\n\r\n\r\ncreate_quantitative_trait <- function(i){\r\n  y <- rnorm(n = 165, mean = 0, sd = 1) \r\n}\r\n\r\ntraits <- as.data.frame(replicate(1000, create_quantitative_trait()))\r\n\r\ntraits_identified <- cbind(hapmap$fam %>%\r\n        select(1:2), traits)\r\n\r\nwrite_delim(traits_identified, \"plinkData/traits_identified\")\r\n\r\n\r\n./plink ‚Äìbfile HapMap_3_r3_1 ‚Äìassoc ‚Äìpheno traits_identified ‚Äìall-pheno ‚Äìpfilter 1e-3\r\n\r\n\r\ndataFiles <- lapply(Sys.glob(\"plinkData/plink.P*.qassoc\"), read_table)\r\n\r\npvalues <- sapply(dataFiles, function(x) min(x$P, na.rm=TRUE))\r\n\r\nas.data.frame(pvalues) %>%\r\n  ggplot(aes(x=pvalues))+\r\n  geom_density(fill = \"cadetblue\")+\r\n  theme_classic()+\r\n  annotate(geom = \"text\", color = \"red\", x = 1e-05, y = 250000, label = \"0.05 quantile:\\n7.31495e-08\", family = \"mono\", cex = 3)+\r\n  geom_vline(xintercept = 7.31495e-08, color = \"red\", linetype = \"dashed\")+\r\n  labs(x=\"P-values\", y = \"Density\", title = \"Distribution of minimum p-values for 1000 replications\")+\r\n  geom_curve(aes(x = 1e-05, y = 270000, xend = 1e-06, yend = 350000), \r\n             arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.3, color = \"red\")+\r\n  theme(plot.title.position = \"plot\", \r\n        plot.title = element_text(family = \"mono\"), \r\n        axis.title = element_text(family = \"mono\"), \r\n        axis.text = element_text(family = \"mono\"))\r\n\r\n\r\nquantile(pvalues, 0.05)\r\n\r\n         5% \r\n7.31495e-08 \r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-29T10:07:28-06:00"
    },
    {
      "path": "HapMapPlink.html",
      "title": "Multiple Hypothesis Testing in PLINK",
      "description": "How to use a more computationally efficient software to determine a threshold.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nRun a GWAS\r\nRun Multiple Hypothesis Testing in PLINK\r\nConclusion and future steps\r\n\r\n\r\n\r\n\r\nNow that we have shown that RStudio is not a computationally efficient software for multiple hypothesis testing, we will show how to do it in PLINK. PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. To learn more about PLINK and how to download and use it, check out this site (‚ÄúWhole Genome Association Analysis Toolset,‚Äù n.d.), which we relied heavily on. There are several different versions of PLINK online - once you download it, you should have a folder containing two black boxes: one called ‚Äúplink‚Äù and one called ‚Äúprettify‚Äù. Once downloaded onto your computer, PLINK runs from the terminal. We can run a GWAS in PLINK in a matter of a couple seconds instead of 30-60 minutes.\r\nThe first step of this is to shown how to run a GWAS in PLINK. Unlike in the example shown using RStudio, we will actually generate a trait of interest based off a randomly selected causal SNP (rs2476601) for this example GWAS.\r\nRun a GWAS\r\nStep 1\r\nSelect pedigree and member columns from the data and bind them to the trait of interest. These are just columns PLINK requires to run a GWAS.\r\n\r\n\r\nset.seed(494)\r\ny = cbind(hapmap$fam %>% select(1:2), trait = X[,'rs2476601'] + rnorm(165, 0, 1))\r\nhead(y)\r\n\r\n        pedigree  member      trait\r\nNA06989     1328 NA06989 -0.7917316\r\nNA11891     1377 NA11891  1.6267809\r\nNA11843     1349 NA11843  1.3422648\r\nNA12341     1330 NA12341  0.1696396\r\nNA12739     1444 NA12739  2.9450827\r\nNA10850     1344 NA10850  2.8350451\r\n\r\nStep 2\r\nWrite the file to the folder with your data and the plink application. After writing the file, the folder should look something like the image below.\r\n\r\n\r\nwrite_delim(y, \"plinkTutorial/trait\")\r\n\r\n\r\n\r\nStep 3\r\nNext, open the folder where your data and the PLINK application are located in terminal. Run the command ./plink --bfile HapMap_3_r3_1 --assoc --adjust --pheno trait --out gwas1. What this is saying is using the HapMap_3_r3_1 files, run an association test with our phenotype of interest trait and send the results to a file called gwas1.qassoc.adjusted. The --adjust means to run one GWAS not accounting for any population structure in the data and one GWAS that does account for population structure (meaning it will make the p-values from the marginal regression models be less inflated, or small). There is not much population structure in this dataset so this is not something we need to worry about/focus on for this analysis.\r\nStep 4\r\nFinally, we read the results back into RStudio and look at most significant SNPs. In a genetic study with a real disease for a trait of interest, these are the SNPs that we would spend time and money studying further and learning more about.\r\n\r\n\r\ngwas1 <- read_table(\"plinkTutorial/gwas1.qassoc.adjusted\") %>%\r\n  arrange(UNADJ)\r\nhead(gwas1)\r\n\r\n# A tibble: 6 √ó 10\r\n    CHR SNP        UNADJ      GC   BONF   HOLM SIDAK‚Ä¶¬π SIDAK‚Ä¶¬≤  FDR_BH\r\n  <dbl> <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\r\n1     1 rs66796‚Ä¶ 8.72e-9 8.72e-9 0.0112 0.0112  0.0111  0.0111 0.00746\r\n2     1 rs24766‚Ä¶ 1.16e-8 1.16e-8 0.0149 0.0149  0.0148  0.0148 0.00746\r\n3     1 rs12022‚Ä¶ 3.45e-8 3.45e-8 0.0442 0.0442  0.0433  0.0433 0.00962\r\n4     1 rs37619‚Ä¶ 3.45e-8 3.45e-8 0.0442 0.0442  0.0433  0.0433 0.00962\r\n5     1 rs23589‚Ä¶ 4.55e-8 4.55e-8 0.0584 0.0584  0.0567  0.0567 0.00962\r\n6     1 rs22737‚Ä¶ 6.19e-8 6.19e-8 0.0794 0.0794  0.0763  0.0763 0.00962\r\n# ‚Ä¶ with 1 more variable: FDR_BY <dbl>, and abbreviated variable\r\n#   names ¬π‚ÄãSIDAK_SS, ¬≤‚ÄãSIDAK_SD\r\n\r\nStep 5\r\nIf we want, we can create a Manhattan plot, which plots the \\(-\\text{log}_{10}(\\text{p-value})\\) of each of SNPs with the trait of interest. The visually helps us see where exactly some of the more significant SNPs are located. The question to ask now is how many SNPs do we look at? Which SNPs might actually be associated with our trait of interest? This is where determining a threshold with multiple hypothesis testing becomes especially useful.\r\n\r\n\r\ngwas1 <- gwas1 %>%\r\n  mutate(CHR = as.integer(CHR)) %>%\r\n  left_join(hapmap$map %>%\r\n              dplyr::select(snp.name, position, chromosome), by = c(\"SNP\" = \"snp.name\", \"CHR\" = \"chromosome\"))\r\n\r\ngwas1 %>%\r\n  mutate(minuslogp = -log10(GC),\r\n         CHR = as.factor(CHR)) %>%\r\n  ggplot(aes(x = CHR, y = minuslogp, group = interaction(CHR, position), color = CHR)) + \r\n  geom_point(position = position_dodge(0.8)) + \r\n  labs(x = 'chromosome', y = expression(paste('-log'[10],'(p-value)')))+\r\n  theme_classic()+\r\n  scale_color_manual(values=natparks.pals(\"DeathValley\",24))+\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nRun Multiple Hypothesis Testing in PLINK\r\nStep 1\r\nThe first step of multiple hypothesis testing in PLINK is to generate a null trait. This example will do 1,000 replications in PLINK, so we will generate 1,000 null traits of normally distributed random noise.\r\n\r\n\r\ncreate_quantitative_trait <- function(i){\r\n  y <- rnorm(n = 165, mean = 0, sd = 1)\r\n}\r\n\r\ntraits <- as.data.frame(replicate(1000, create_quantitative_trait()))\r\n\r\n\r\nStep 2\r\nNext, we select the member and pedigree information on each individual and column bind on the 1,000 null traits. This creates a table with one row for each person (165 rows) and 1,0002 columns (member, pedigree, plus the 1000 traits).\r\n\r\n\r\ntraitReps <- cbind(hapmap$fam %>%\r\n        dplyr::select(1:2), traits)\r\n\r\n\r\nStep 3\r\nWrite the traitReps table to the same folder where the data and the PLINK application is stored.\r\n\r\n\r\nwrite_delim(traitReps, \"plinkTutorial/traitReps\")\r\n\r\n\r\nStep 4\r\nOpen the folder where the data and the PLINK application are located in terminal. Run the command ./plink --bfile HapMap_3_r3_1 --assoc --pheno traitReps --all-pheno --pfilter 1e-3. What this is saying is using the HapMap_3_r3_1 files, run one association test with each of our 1000 null traits in the traitReps file. Only keep p-values that are smaller than \\(1 \\times 10^{-3}\\) to limit the size of the files created. This will create 1000 files titled plink.P*.qassoc in the plinkTutorial folder, where the * is the number of the file.\r\nStep 5\r\nThen read the 1,000 file into RStudio. It depends on the size of the files how long this takes, but for this dataset it takes about 3 minutes.\r\n\r\n\r\ndataFiles <- lapply(Sys.glob(\"plinkTutorial/plink.P*.qassoc\"), read_table)\r\n\r\n\r\nStep 6\r\nFinally, take the smallest p-value from each of the 1,000 genetic-wide association studies, and then take the 5% quantile (desired FWER) of those 1000 smallest p-values. This gives us our threshold. We got \\(3.39 \\times 10^{-8}\\).\r\n\r\n\r\npvalues <- sapply(dataFiles, function(x) min(x$P, na.rm=TRUE))\r\nquantile(pvalues, 0.05)\r\n\r\n\r\nWe can then go back to our original GWAS and see which SNPs have p-values below this threshold of \\(3.39 \\times 10^{-8}\\). As shown in the manhattan plot below, there are two SNPs with p-values smaller than our threshold. One of these is our causal SNP rs2476601, and the other is a neighboring SNP that likely contains very similar information to our causal SNP. In a true genetic study, we would not know if either of these is truly associated with our trait of interest until we devote further time and resources into studying them.\r\n\r\nConclusion and future steps\r\nPLINK has allowed us to efficiently (a) run a GWAS to find important relationships between genetic data and a trait of interest and (b) determine which SNPs from that GWAS to devote further time and resources to. In the future, we would like to do this process with a more diverse set of data, which would require us to learn the additional steps necessary for accounting for the confounding effects of family and population structure in genetic data.\r\n\r\n\r\n\r\n‚ÄúWhole Genome Association Analysis Toolset.‚Äù n.d. PLINK: Whole Genome Data Analysis Toolset. https://zzz.bwh.harvard.edu/plink/.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-10T14:40:13-06:00"
    },
    {
      "path": "index.html",
      "title": "An Extension of Multiple Hypothesis Testing in Statistical Genetics",
      "description": "A project by Will Brazgel and Erin Franke exploring multiple hypothesis testing and using RStudio and PLINK to efficiently execute it on genetic data.\n",
      "author": [],
      "contents": "\r\nOn this website, we will introduce what multiple hypothesis testing is and why it is necessary for genetic data. We will then show an example of how to complete multiple hypothesis testing in both RStudio and PLINK. Our process should be relatively easy to follow for anyone that has taken an introductory statistics class. We recommend starting on the Multiple Hypothesis Testing tab and moving to the tabs on the right from there. Thanks for following along!\r\n\r\nAcknowledgements\r\nSpecial thanks to our professor Kelsey Grinde for her support through this project!\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-10T17:15:44-06:00"
    },
    {
      "path": "multipletesting.html",
      "title": "Multiple Hypothesis Testing",
      "description": "An explanation of the complexities to making statistically significant conclusions when working with genetic data.",
      "author": [],
      "contents": "\r\n\r\nContents\r\nGenome Wide Association Studies\r\nHypothesis Testing in GWAS\r\nFalse Discovery Rate\r\n\r\nGenome Wide Association Studies\r\nTo understand the need for multiple hypothesis testing, we need to be familiar with Genome Wide Association Studies(GWAS) and the complexities to working with genetic data.\r\nExplanation of GWAS\r\nIf we compare any two human genomes, they are nearly identical. However, places where DNA sequences differ are known as genetic variants. Once there is a collection of variants to be studied, GWAS attempts to determine which genetic variants are associated with an illness (Type One Diabetes, Crohn‚Äôs Disease) or trait of interest (height, blood pressure).\r\nThere are multiple types of genetic variants, but GWAS is focused on variants that are exchanges of one nucleotide base pair for another, otherwise known as single nucleotide polymorphisms(SNPs). Figure 1 is a visual example of a SNP.\r\nSNPs : DNA variations that occur when a single nucleotide base pair in the genome sequence is altered (AT vs GC at a single position).\r\n\r\nFigure 1: SNP Example\r\nWorking with Genetic Data\r\nWhen working with genetic data and running GWAS, the number of SNPs to be studied is commonly greater than 1,000,000. Because of this, we cannot simply fit a multiple linear regression model to predict the existence of an illness or the value of a trait of interest. As an equation, this would be equivalent to E[y | x] = Íûµ + Íûµ1x + ‚Ä¶ + Íûµnx, where y is the trait and x is the number of minor alleles.\r\nIn this situation, our goal would be to find a line of best fit where ‚Äúbest‚Äù is described as the minimized sum of squared residuals, which requires complicated derivations. This would be quite time consuming and difficult to find the optimal Íûµ0 and Íûµ1‚Ä¶Íûµn considering how large the number of predictors is. Even if we used a different technique such as matrix derivation, because there are more SNPs(columns) than individuals we are studying(rows), our matrix is not invertible and thus our least squares estimator is not defined. This conclusion is proven through linear algebra.\r\nInstead, we utilize Marginal Regression. As an equation, this would be equivalent to E[y | xj] = Íûµ + Íûµ1xj + ‚Ä¶ + Íûµnxj, where y is the trait and x is the number of minor alleles at that SNP. From these marginal regressions, we receive test statistics and p-values for each SNP. Once we have a p-value for each SNP, we can begin to visualize these values and make decisions of whether or not a SNP is associated with our trait of interest. Figure 2 is an example of how we can visualize p-values, known as a Manhattan Plot. The y-axis is -log10 transformed so that smaller p-values appear as higher on the plot. For now, ignore the horizontal blue line.\r\n\r\nFigure 2: Manhattan Plot\r\nHypothesis Testing in GWAS\r\nIn order to make these decisions of whether or not an association exists, we utilize hypothesis testing. As you read this section, note that the null hypothesis in GWAS is that there is no relationship between a SNP and the trait of interest.\r\nHypothesis Testing Review\r\nTo review, hypothesis testing is predicated on the comparison of a p-value and a predetermined significance threshold, commonly represented as ùõº and equal to 0.05. There are two hypotheses, the null hypothesis and the alternative hypothesis. If a p-value is less than the significance threshold, we reject the null hypothesis and therefore conclude the alternative hypothesis to be true. In context, we would conclude that there is a relationship between that SNP and the trait of interest. On the other hand, if a p-value is greater than the significance threshold, we cannot reject the null hypothesis and therefore conclude the null hypothesis to be true. In context, we would conclude that there is not a relationship between that SNP and the trait of interest. Figure 3 is a table that summarizes the decisions we can make and the consequences of those decisions. We desire to correctly reject the null as much as possible(power) while avoiding both falsely rejecting the null(Type 1 Error) and failing to reject the null when we should (Type 2 Error).\r\n\r\nFigure 3: Hypothesis Testing Decisions\r\nThe Multiple Testing Problem\r\nAside from the added GWAS context, our explanation should be identical to what you would encounter in an introductory statistics course. However, there is additional complexity in GWAS because of the aforementioned need for marginal regression. Recall, we are not running one hypothesis test, but millions, and this drastically effects our ability to conclude that a SNP is significantly associated.\r\nTo illustrate, imagine there is a bucket with ten balls inside, nine are red and one is blue, as seen in Figure 4. If you reach in the bucket once, your chance of grabbing a red ball is 90% (9/10). If you reach in the bucket five times, independently, your chance of grabbing a red ball all five times is 59% (.9^5). In this five-ball scenario, your chance of grabbing the blue ball at least once increases from 10%(one-ball scenario) to 41%. As expected, if you reach in the bucket ten times, independently, your chance of grabbing a red ball all ten times is even smaller, 35% (.9^10), and your chance of grabbing the blue ball at least once increases to 65%. Overall, the more times you reach in the bucket, the higher the chance of grabbing at least one blue ball.\r\n\r\nFigure 4: Bucket Example\r\nWith this in mind, consider every time you reach into the bucket as a hypothesis test and the event of grabbing the blue ball as Type 1 Error. As the number of hypothesis tests increases, the chance of at least one Type 1 Error increases. In words, if we reject the null hypothesis when we witness a ‚Äúrare‚Äù event, it will be easier to find rare events and furthermore easier to mistakenly think that an event is truly rare as the number of tests get increasingly larger. Clearly, this is a major problem with the multiple hypothesis testing procedure that is needed to make decisions in GWAS.\r\nThis concept is further supported by this equation, P(at least one Type 1 Error) = 1 - (1 - ùõº)^n, which holds due to a number of proofs that we will point out but won‚Äôt fully explain at this time. First, according to mathematical statistics proofs, ùõº is equal to the chance of Type 1 Error for a singular hypothesis test. Second, according to probability proofs, P(A >= 1) = 1 - P(complement of A) where A = Type 1 Error. Lastly, we can raise our equation to the power of n, with n being the number of hypothesis tests we are running, because we assume each test to be independent.\r\nDetermining a Threshold\r\nThis probability of at least one Type 1 Error is better known as the Family Wise Error Rate (FWER). Rather than ùõº = 0.05, we often want FWER = 0.05. Looking back at the equation for FWER, we know the number of tests to be conducted will be over a million, so we must adjust the significance threshold.\r\nTo do this, there is a widely used approach known as the Bonferroni Correction. This correction is very straightforward, simply dividing the desired FWER by the number of hypothesis tests conducted. Here is the Bonferroni Correction as an equation. ùõº = FWER / Number of Hypothesis Tests. For example, if we conduct 1,000,000 tests and would like a FWER of 0.05, ùõº must equal 5 x 10 ^ -8. With this new threshold in mind, let‚Äôs look back at a Manhattan Plot of SNPs that are truly null. If these SNPs are truly null, any point above the horizontal threshold line is considered a Type 1 Error. Figure 5 has two horizontal lines, one at ùõº = 0.05 and one at ùõº = 0.00000005. Imagine the proportion of Type 1 Error if our threshold was not adjusted.\r\n\r\nFigure 5: Manhattan Plot With Thresholds\r\nFalse Discovery Rate\r\nWhile the Bonferroni Correction is straightforward, easy to use, and clearly effective (as shown by Figure 5), there are limitations and shortcomings to this procedure. When hypothesis tests are correlated, which they commonly are in GWAS, the Bonferroni Correction is overly conservative. This means that the adjusted significance threshold is too small, so we will not be able to reject the null as much, limiting power. Without diving into how and why SNPs are correlated, this truth about Bonferroni can be understood by analyzing the equation. When SNPs are correlated, there are essentially less SNPs being studied, and because we divide by the number of hypothesis tests conducted, this number in the denominator is larger than it should be, causing ùõº to be smaller than it should be.\r\nTo account for correlation, there are simulation-based approaches to determining a threshold. For more about this, please continue to read through this website.\r\nBesides simulation-based approaches, there are also alternatives to the FWER. One of these alternatives is known as the False Discovery Rate (FDR). FDR is defined as the proportion of false rejections of the null among all significant results. A FDR of 5% means that, among all features called significant, 5% of these are truly null. Essentially, the FWER controls the probability of all Type 1 Error, while the FDR allows more Type 1 Error but controls how many there are in proportion to true rejections of the null. While FDR may allow for a higher Type 1 Error rate, the trade-off is higher power. Furthermore, this power advantage increases with an increasing number of hypothesis tests.\r\nFor more on the FDR compared to the FWER, check out this journal. FDR Journal\r\n\r\n\r\n\r\n",
      "last_modified": "2024-02-21T17:23:46-06:00"
    }
  ],
  "collections": []
}
